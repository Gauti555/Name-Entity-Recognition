# -*- coding: utf-8 -*-
"""NER_ALL .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16t_-ReczgwG8dH3bwd9aQ1eANcaqpBql
"""

import pandas as pd
import numpy as np
import os 
import re
import operator
import pickle
import nltk 
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import drive

drive.mount("/content/drive")

article = pd.read_json(r'/content/drive/My Drive/file.json /GoogleSearchResults.json')
article.reset_index(inplace=True)
article.drop(columns='index')

import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()

clean_keyword=[]
for i in range (len(article)):
    clean_keyword.append(article.iloc[i,4])

print(clean_keyword)

name_entity=[]
for i in range (len(article)):
    clean_keyword.append(article.iloc[i,4])

for i in range (len(article)):
  doc = nlp(article.iloc[i,4])
  print([(X.text, X.label_) for X in doc.ents])

